# GenAIGlossary
A glossary of terms in Generative AI and their details.

## Back story
It's becoming difficult to catch up with the AI space. This doc will can be used as a refresher to keep track of the terms and their details in the Generative AI space.

## Glossary
| Term | Description            | Source     |
| ---- | :--------------------- | :--------- |
|AGI        |	Artificial general intelligence, A concept that suggests a more advanced version of AI than we know today, one that can perform tasks much better than humans while also teaching and advancing its own capabilities.	
|BART       |	Bidirectional and Auto-Regressive Transformers	                                    |       https://arxiv.org/pdf/1910.13461.pdf
|BELEBELE   |   A Bambara word meaning "big, large, fat, great". This is a dataset containing 900 unique multiple-choice reading comprehension questions, each associated with one of 488 distinct passages  |       https://arxiv.org/pdf/2308.16884.pdf
|BERT       |	Bidirectional Encoder Representations from Transformers	                            |       https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766
|BLOOM      |	BigScience Large Open-science Open-access Multilingual Language Model	            |       https://bigscience.huggingface.co/
|BPE        |	Byte Pair Encoding	                                                                |       https://en.wikipedia.org/wiki/Byte_pair_encoding
|C4         |   The Colossal Clean Crawled Corpus, a dataset of 800GB of English text collected from the web.  |  https://www.tensorflow.org/datasets/catalog/c4
|CLIP       |	Contrastive Language-Image Pre-training	                                            |       https://openai.com/blog/clip/
|DALL-E     |	It is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dal√≠.	    |       https://openai.com/research/dall-e
|DPR        |	Dense Passage Retrieval	                                                            |       https://arxiv.org/pdf/2004.04906.pdf
|ELMo       |   Embeddings from Language Models                                                     |       https://arxiv.org/pdf/1802.05365v2.pdf
|ERNIE      |	Enhanced Representation through kNowledge IntEgration	                            |       https://arxiv.org/abs/1904.09223
|ELECTRA    |	Efficiently Learning an Encoder that Classifies Token Replacements Accurately	    |       https://arxiv.org/pdf/2003.10555.pdf
|FAIR       |	Facebook AI Research	
|FLAN       |	Fine tuning Language models	                                                        |       https://arxiv.org/pdf/2210.11416.pdf
|FLOPS      |	Floating Point Operations Per Second	                                            |       https://en.wikipedia.org/wiki/FLOPS
|FLoRes     |	*F*acebook *Lo*w *Res* Machine Translation Benchmark is a low-resource MT dataset.  |       https://github.com/facebookresearch/flores
|GGML       |   Georgi Gerganov Machine Learning, a C library focused on machine learning           |       https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML
|GSM8K      |	Grade School Math 8K, GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. |  https://arxiv.org/pdf/2109.01152.pdf
|HNSW       |	Hierarchical Navigable Small Worlds	                                                |       https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf
|LaMDA      |   Language Model for Dialogue Applications	                                        |       https://arxiv.org/pdf/2201.08239.pdf
|LLaMA      |	Large Language Model Meta AI	                                                    |       https://ai.meta.com/blog/large-language-model-llama-meta-ai/
|LLaSM      |   Large Language and Speech Model                                                     |       https://arxiv.org/pdf/2308.15930.pdf
|LLM        |	Large Language Model: An AI model trained on mass amounts of text data to understand language and generate novel content in human-like language.	
|MAWPS      |	A Math Word Problem Repository is an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms. | https://github.com/sroy9/mawps
|ML         |	Machine Learning, A component in AI that allows computers to learn and make better predictive outcomes without explicit programming. Can be coupled with training sets to generate new content.	
|MLM        |	Masked Language Model	
|MRC        |   Machine Reading Comprehension  
|MTPB       |   Multi-Turn Programming Benchmark, a benchmark consisting of 115 diverse problem sets that are factorized into multi-turn prompts | https://arxiv.org/pdf/2203.13474.pdf
|NeurIPS    |	Neural Information Processing System	                                            |       https://neurips.cc/
|NLP        |	Natural language processing. A branch of AI that uses machine learning and deep learning to give computers the ability to understand human language, often using learning algorithms, statistical models and linguistic rules.	
|NLG        |	Natural Language Generation. A branch of AI that uses machine learning and deep learning to generate human-like language.	
|NSP        |	Next Sentence Prediction
|PALM       |   Pathways Language Model	                                                            |       https://arxiv.org/pdf/2204.02311.pdf
|PEFT       |	Parameter Efficient Fine-Tuning	                                                    |       https://huggingface.co/blog/peft
|PPO        |	Proximal Policy Optimization, foundational RL algorithm for learning from human preferences |       https://arxiv.org/pdf/1707.06347.pdf
|RAG        |   Retriever-Augmented Generation is an AI framework that combines an information retrieval component with a text generation model to improve the quality of responses generated by LLMs.      |       https://arxiv.org/pdf/2005.11401.pdf
|RLHF       |	Reinforcement Learning from Human Feedback	                                        |       https://arxiv.org/pdf/2305.18438.pdf
|RoBERTa    |	Robustly Optimized BERT Approach	                                                |       https://arxiv.org/pdf/1907.11692.pdf
|SQuAD      |	Stanford Question Answering Dataset	                                                |       https://arxiv.org/pdf/1606.05250.pdf
|SVAMP      |   Simple Variations on Arithmetic Math word Problems is a challenge set to enable more robust evaluation of automatic MWP (Math Word Problem) solvers | https://github.com/arkilpatel/SVAMP
|T5         |	Text to Text Transfer Transformer	                                                |       https://huggingface.co/docs/transformers/model_doc/t5
|XLM        |	Cross-lingual Language Models	                                                    |       https://arxiv.org/pdf/1901.07291.pdf
|XLU        |	Cross-lingual Understanding	
|XNLI       |	Cross-lingual Natural Language Inference	                                        |       https://github.com/facebookresearch/XNLI
|XLNet      |	Generalized Autoregressive Pretraining for Language Understanding	                |       https://arxiv.org/pdf/1906.08237.pdf
|ZeRO       |   *Ze*ro *R*edundancy *O*ptimizer	                                                    |       https://arxiv.org/pdf/1910.02054.pdf

---
Note: _PRs are accepted. Feel free to add more terms and their details._