# GenAIGlossary
A glossary of terms in Generative AI and their details.

## Back story
It's becoming difficult to catch up with the AI space. This doc will can be used as a refresher to keep track of the terms and their details in the Generative AI space.

## Glossary
| Term | Description            | Source     |
| ---- | :--------------------- | :--------- |
|AGI        |	Artificial general intelligence, A concept that suggests a more advanced version of AI than we know today, one that can perform tasks much better than humans while also teaching and advancing its own capabilities.	
|BART       |	Bidirectional and Auto-Regressive Transformers	                                    |       https://arxiv.org/pdf/1910.13461.pdf
|BELEBELE   |   A Bambara word meaning "big, large, fat, great". This is a dataset containing 900 unique multiple-choice reading comprehension questions, each associated with one of 488 distinct passages  |       https://arxiv.org/pdf/2308.16884.pdf
|BERT       |	Bidirectional Encoder Representations from Transformers	                            |       https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766
|BLOOM      |	BigScience Large Open-science Open-access Multilingual Language Model	            |       https://bigscience.huggingface.co/
|BPE        |	Byte Pair Encoding	                                                                |       https://en.wikipedia.org/wiki/Byte_pair_encoding
|CLIP       |	Contrastive Language-Image Pre-training	                                            |       https://openai.com/blog/clip/
|DALL-E     |	It is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dal√≠.	    |       https://openai.com/research/dall-e
|DPR        |	Dense Passage Retrieval	                                                            |       https://arxiv.org/pdf/2004.04906.pdf
|ERNIE      |	Enhanced Representation through kNowledge IntEgration	                            |       https://arxiv.org/abs/1904.09223
|ELECTRA    |	Efficiently Learning an Encoder that Classifies Token Replacements Accurately	    |       https://arxiv.org/pdf/2003.10555.pdf
|FAIR       |	Facebook AI Research	
|FLAN       |	Fine tuning Language models	                                                        |       https://arxiv.org/pdf/2210.11416.pdf
|FLOPS      |	Floating Point Operations Per Second	                                            |       https://en.wikipedia.org/wiki/FLOPS
|FLoRes     |	*F*acebook *Lo*w *Res* Machine Translation Benchmark is a low-resource MT dataset.  |       https://github.com/facebookresearch/flores
|HNSW       |	Hierarchical Navigable Small Worlds	                                                |       https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf
|LaMDA      |   Language Model for Dialogue Applications	                                        |       https://arxiv.org/pdf/2201.08239.pdf
|LLaMA      |	Large Language Model Meta AI	                                                    |       https://ai.meta.com/blog/large-language-model-llama-meta-ai/
|LLaSM      |   Large Language and Speech Model                                                     |       https://arxiv.org/pdf/2308.15930.pdf
|LLM        |	Large Language Model: An AI model trained on mass amounts of text data to understand language and generate novel content in human-like language.	
|ML         |	Machine Learning, A component in AI that allows computers to learn and make better predictive outcomes without explicit programming. Can be coupled with training sets to generate new content.	
|MLM        |	Masked Language Model	
|MRC        |   Machine Reading Comprehension  
|NeurIPS    |	Neural Information Processing System	                                            |       https://neurips.cc/
|NLP        |	Natural language processing. A branch of AI that uses machine learning and deep learning to give computers the ability to understand human language, often using learning algorithms, statistical models and linguistic rules.	
|NLG        |	Natural Language Generation. A branch of AI that uses machine learning and deep learning to generate human-like language.	
|NSP        |	Next Sentence Prediction
|PALM       |   Pathways Language Model	                                                            |       https://arxiv.org/pdf/2204.02311.pdf
|PEFT       |	Parameter Efficient Fine-Tuning	                                                    |       https://huggingface.co/blog/peft
|RAG        |   Retriever-Augmented Generation is an AI framework that combines an information retrieval component with a text generation model to improve the quality of responses generated by LLMs.      |       https://arxiv.org/pdf/2005.11401.pdf
|RoBERTa    |	Robustly Optimized BERT Approach	                                                |       https://arxiv.org/pdf/1907.11692.pdf
|SQuAD      |	Stanford Question Answering Dataset	                                                |       https://arxiv.org/pdf/1606.05250.pdf
|T5         |	Text to Text Transfer Transformer	                                                |       https://huggingface.co/docs/transformers/model_doc/t5
|XLM        |	Cross-lingual Language Models	                                                    |       https://arxiv.org/pdf/1901.07291.pdf
|XLU        |	Cross-lingual Understanding	
|XNLI       |	Cross-lingual Natural Language Inference	                                        |       https://github.com/facebookresearch/XNLI
|XLNet      |	Generalized Autoregressive Pretraining for Language Understanding	                |       https://arxiv.org/pdf/1906.08237.pdf

---
Note: _PRs are accepted. Feel free to add more terms and their details._