# AIGlossary
A glossary of terms in AI and their details.

## Back story
It's becoming difficult to catch up with the AI space. This doc will can be used as a refresher to keep track of the terms and their details in the AI space.

## Glossary

### A - M
| Term                                                                         | Description                                                                                                                                                                                                                                                                                               |
| ---------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [A100](https://en.wikipedia.org/wiki/Ampere_%28microarchitecture%29)         | Ampere 100, A GPU variant named after French mathematician and physicist André-Marie Ampère                                                                                                                                                                                                               |
| AGI                                                                          | Artificial general intelligence, A concept that suggests a more advanced version of AI than we know today, one that can perform tasks much better than humans while also teaching and advancing its own capabilities.                                                                                     |
| [ALIGN](https://blog.research.google/2021/05/align-scaling-up-visual-and-vision.html) | **A** **L**arge-scale **I**ma**G**e and **N**oisy-Text Embedding, 1.8 Billion Image-Text pairs dataset by Google.
| [BART](https://arxiv.org/pdf/1910.13461.pdf)                                 | Bidirectional and Auto-Regressive Transformers                                                                                                                                                                                                                                                            |
| [BELEBELE](https://arxiv.org/pdf/2308.16884.pdf)                             | A Bambara word meaning "big, large, fat, great". This is a dataset containing 900 unique multiple-choice reading comprehension questions, each associated with one of 488 distinct passages                                                                                                               |
| [BERT](https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766) | Bidirectional Encoder Representations from Transformers                                                                                                                                                                                                                                                   |
| [BIG-Bench](https://openreview.net/forum?id=uyTL5Bvosj)                      | **B**eyond the **I**mitation **G**ame **Bench**mark, a benchmark for measuring the performance of language models across a diverse set of tasks.                                                                                                                                                          |
| [BiT](https://arxiv.org/abs/1912.11370)                                      | Big Transfer, a family of transfer learning models pre-trained on large datasets.                                                                                                                                                                                                                         |
| [BLEU](https://en.wikipedia.org/wiki/BLEU)                                   | Bilingual Evaluation Understudy, a metric for evaluating a generated sentence to a reference sentence.                                                                                                                                                                                                    |
| [BLOOM](https://bigscience.huggingface.co/)                                  | BigScience Large Open-science Open-access Multilingual Language Model                                                                                                                                                                                                                                     |
| [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)                      | Byte Pair Encoding, a tokenization method.                                                                                                                                                                                                                                                                |
| [C4](https://www.tensorflow.org/datasets/catalog/c4)                         | The Colossal Clean Crawled Corpus, a dataset of 800GB of English text collected from the web.                                                                                                                                                                                                             |
| [Chinchilla](https://arxiv.org/abs/2203.15556)                               | Chinchilla is a 70B parameters model trained as a compute-optimal model with 1.4 trillion tokens by Deepmind.                                                                                                                                                                                                                                                     |
| [CLIP](https://openai.com/blog/clip/)                                        | Contrastive Language-Image Pre-training, maps data of different modalities, text and images, into a shared embedding space.                                                                                                                                                                               |
| [DALL-E](https://openai.com/research/dall-e)                                 | It is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dalí.                                                                                                                                                                                |
| [DPR](https://arxiv.org/pdf/2004.04906.pdf)                                  | Dense Passage Retrieval                                                                                                                                                                                                                                                                                   |
| [ELMo](https://arxiv.org/pdf/1802.05365v2.pdf)                               | Embeddings from Language Models                                                                                                                                                                                                                                                                           |
| [ERNIE](https://arxiv.org/abs/1904.09223)                                    | **E**nhanced **R**epresentation through k**N**owledge **I**nt**E**gration                                                                                                                                                                                                                                                     |
| [ELECTRA](https://arxiv.org/pdf/2003.10555.pdf)                              | Efficiently Learning an Encoder that Classifies Token Replacements Accurately                                                                                                                                                                                                                             |
| FAIR                                                                         | Facebook AI Research                                                                                                                                                                                                                                                                                      |
| [FLAN](https://arxiv.org/pdf/2210.11416.pdf)                                 | Fine tuning Language models                                                                                                                                                                                                                                                                               |
| [FLOPS](https://en.wikipedia.org/wiki/FLOPS)                                 | Floating Point Operations Per Second                                                                                                                                                                                                                                                                      |
| [FLoRes](https://github.com/facebookresearch/flores)                         | **F**acebook **Lo**w **Res** Machine Translation Benchmark is a low-resource MT dataset.                                                                                                                                                                                                                        |
| [GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)                | Georgi Gerganov Machine Learning, a C library focused on machine learning                                                                                                                                                                                                                                 |
| [GLaM](https://arxiv.org/abs/2112.06905v2)                                   | Generalist Language Model, a family of language models which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants.                                                                      |
| [GSM8K](https://arxiv.org/pdf/2109.01152.pdf)                                | Grade School Math 8K, GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers.                                                                                                                                                    |
| [HNSW](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)               | Hierarchical Navigable Small Worlds                                                                                                                                                                                                                                                                       |
| [ILSVRC2012](https://www.image-net.org/challenges/LSVRC/2012/)               | ImageNet Large Scale Visual Recognition Challenge 2012, a competition to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training. |
| [JFT](https://arxiv.org/abs/1503.02531.pdf)                                  | JFT-300M is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback.                                                                                |
| [LAION-400M](https://arxiv.org/abs/2111.02114.pdf)                           | Large-scale Artificial Intelligence Open Network, an open dataset of CLIP-Filtered 400 Million Image-Text Pairs
| [LaMDA](https://arxiv.org/pdf/2201.08239.pdf)                                | Language Model for Dialogue Applications                                                                                                                                                                                                                                                                  |
| [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)        | Large Language Model Meta AI                                                                                                                                                                                                                                                                              |
| [LLaSM](https://arxiv.org/pdf/2308.15930.pdf)                                | Large Language and Speech Model                                                                                                                                                                                                                                                                           |
| LLM                                                                          | Large Language Model: An AI model trained on mass amounts of text data to understand language and generate novel content in human-like language.                                                                                                                                                          |
| [LLaVA](https://arxiv.org/abs/2304.08485)                                    | Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general purpose visual and language understanding.                                                                                                                           |
| [LMM](https://arxiv.org/abs/2204.14198)                                      | Large Multimodal Models, models for visual instructions like DeepMind’s Flamingo, Google’s PaLM-E, Salesforce’s BLIP, Microsoft’s KOSMOS-1, Tencent’s Macaw-LLM; Chatbots like ChatGPT and Gemini are LMMs.                                                                                               |
| [M3W](https://arxiv.org/pdf/2204.14198.pdf)                                  | Multi Modal Massive Web, an image and text dataset by DeepMind. This is used to train Flamingo, a multimodal LLM.                                                                                                                                                                                        |
| [MAWPS](https://github.com/sroy9/mawps)                                      | A Math Word Problem Repository is an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms.                                                                                                                                                              |
| ML                                                                           | Machine Learning, A component in AI that allows computers to learn and make better predictive outcomes without explicit programming. Can be coupled with training sets to generate new content.                                                                                                           |
| MLP                                                                          | Multi Level Perceptron, a deep artificial neural network. It is a collection of more than one perceptron.                                                                                                                                                                                                 |
| MLM                                                                          | Masked Language Model                                                                                                                                                                                                                                                                                     |
| [MMLU](https://arxiv.org/abs/2009.03300)                                     | Massive Multitask Language Understanding, a new test to measure a text model's multitask accuracy                                                                                                                                                                                                         |
| MRC                                                                          | Machine Reading Comprehension                                                                                                                                                                                                                                                                             |
| [MTPB](https://arxiv.org/pdf/2203.13474.pdf)                                 | Multi-Turn Programming Benchmark, a benchmark consisting of 115 diverse problem sets that are factorized into multi-turn prompts                                                                                                                                                                          |

### N - Z
| Term                                                                         | Description                                                                                                                                                                                                                                                                                               |
| ---------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [NeurIPS](https://neurips.cc/)                                               | Neural Information Processing System                                                                                                                                                                                                                                                                      |
| NLP                                                                          | Natural language processing. A branch of AI that uses machine learning and deep learning to give computers the ability to understand human language, often using learning algorithms, statistical models and linguistic rules.                                                                            |
| NLG                                                                          | Natural Language Generation. A branch of AI that uses machine learning and deep learning to generate human-like language.                                                                                                                                                                                 |
| NLU                                                                          | Natural Language Understanding, to understand the relationship and meaning in text data.                                                                                                                                                                                                                  |
| NSP                                                                          | Next Sentence Prediction                                                                                                                                                                                                                                                                                  |
| [PALM](https://arxiv.org/pdf/2204.02311.pdf)                                 | Pathways Language Model                                                                                                                                                                                                                                                                                   |
| [PEFT](https://huggingface.co/blog/peft)                                     | Parameter Efficient Fine-Tuning                                                                                                                                                                                                                                                                           |
| [PPO](https://arxiv.org/pdf/1707.06347.pdf)                                  | Proximal Policy Optimization, foundational RL algorithm for learning from human preferences                                                                                                                                                                                                               |
| [RAG](https://arxiv.org/pdf/2005.11401.pdf)                                  | Retriever-Augmented Generation is an AI framework that combines an information retrieval component with a text generation model to improve the quality of responses generated by LLMs.                                                                                                                    |
| [ResNet](https://arxiv.org/abs/1512.03385)                                   | A Residual Neural Network (a.k.a. Residual Network, ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs.                                                                                                                              |
| [RLHF](https://arxiv.org/pdf/2305.18438.pdf)                                 | Reinforcement Learning from Human Feedback                                                                                                                                                                                                                                                                |
| [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)                              | Robustly Optimized BERT Approach                                                                                                                                                                                                                                                                          |
| [ROGUE](https://arxiv.org/abs/1803.01937)                                    | Recall-Oriented Understudy for Gisting Evaluation, a metric for evaluating a generated sentence to a reference sentence.                                                                                                                                                                                  |
| SFT                                                                          | Supervised Fine Tuning, A fine-tuning method especially in LLMs                                                                                                                                                                                                                                           |
| [SQuAD](https://arxiv.org/pdf/1606.05250.pdf)                                | Stanford Question Answering Dataset                                                                                                                                                                                                                                                                       |
| [SVAMP](https://github.com/arkilpatel/SVAMP)                                 | Simple Variations on Arithmetic Math word Problems is a challenge set to enable more robust evaluation of automatic MWP (Math Word Problem) solvers                                                                                                                                                       |
| [T5](https://huggingface.co/docs/transformers/model_doc/t5)                  | Text to Text Transfer Transformer                                                                                                                                                                                                                                                                         |
| [ViT](https://arxiv.org/abs/2010.11929)                                      | Vision Transformer, a vision model based as closely as possible on the Transformer architecture originally designed for text-based tasks.
| VLU                                                                          | Vision Language Understanding, like Natural Language Understanding (NLU) but for images                                                                                                                                                                                                                   |
| [VRAM](https://en.wikipedia.org/wiki/Video_random-access_memory)             | Video Random Access Memory, a special type of memory that stores graphics data for the GPU.                                                                                                                                                                                                               |
| [XLM](https://arxiv.org/pdf/1901.07291.pdf)                                  | Cross-lingual Language Models                                                                                                                                                                                                                                                                             |
| XLU                                                                          | Cross-lingual Understanding                                                                                                                                                                                                                                                                               |
| [XNLI](https://github.com/facebookresearch/XNLI)                             | Cross-lingual Natural Language Inference                                                                                                                                                                                                                                                                  |
| [XLNet](https://arxiv.org/pdf/1906.08237.pdf)                                | Generalized Autoregressive Pretraining for Language Understanding                                                                                                                                                                                                                                         |
| [ZeRO](https://arxiv.org/pdf/1910.02054.pdf)                                 | **Ze**ro **R**edundancy **O**ptimizer                                                                                                                                                                                                                                                                           |

---
Note: _PRs are accepted. Feel free to add more terms and their details._