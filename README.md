# AIGlossary
A glossary of terms in AI and their details.

## Back story
It's becoming difficult to catch up with the AI space. This doc will can be used as a refresher to keep track of the terms and their details in the AI space.

## Glossary
| Term                                              | Description                                                        |
| ------------------------------------------------- | :----------------------------------------------------------------- |
|AGI        |	Artificial general intelligence, A concept that suggests a more advanced version of AI than we know today, one that can perform tasks much better than humans while also teaching and advancing its own capabilities.	
|[BART](https://arxiv.org/pdf/1910.13461.pdf)       |	Bidirectional and Auto-Regressive Transformers	                                    |
|[BELEBELE](https://arxiv.org/pdf/2308.16884.pdf)   |   A Bambara word meaning "big, large, fat, great". This is a dataset containing 900 unique multiple-choice reading comprehension questions, each associated with one of 488 distinct passages  |       
|[BERT](https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766)       |	Bidirectional Encoder Representations from Transformers	                            |       
|[BLEU](https://en.wikipedia.org/wiki/BLEU)       |	Bilingual Evaluation Understudy, a metric for evaluating a generated sentence to a reference sentence. |       
|[BLOOM](https://bigscience.huggingface.co/)      |	BigScience Large Open-science Open-access Multilingual Language Model	            |       
|[BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)        |	Byte Pair Encoding	                                                                |       
|[C4](https://www.tensorflow.org/datasets/catalog/c4)         |   The Colossal Clean Crawled Corpus, a dataset of 800GB of English text collected from the web.  |  
|[CLIP](https://openai.com/blog/clip/)       |	Contrastive Language-Image Pre-training	                                            |       
|[DALL-E](https://openai.com/research/dall-e)     |	It is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dal√≠.	    |       
|[DPR](https://arxiv.org/pdf/2004.04906.pdf)        |	Dense Passage Retrieval	                                                            |       
|[ELMo](https://arxiv.org/pdf/1802.05365v2.pdf)       |   Embeddings from Language Models                                                     |       
|[ERNIE](https://arxiv.org/abs/1904.09223)      |	Enhanced Representation through kNowledge IntEgration	                            |       
|[ELECTRA](https://arxiv.org/pdf/2003.10555.pdf)    |	Efficiently Learning an Encoder that Classifies Token Replacements Accurately	    |       
|FAIR       |	Facebook AI Research	
|[FLAN](https://arxiv.org/pdf/2210.11416.pdf)       |	Fine tuning Language models	                                                        |       
|[FLOPS](https://en.wikipedia.org/wiki/FLOPS)      |	Floating Point Operations Per Second	                                            |       
|[FLoRes](https://github.com/facebookresearch/flores)     |	*F*acebook *Lo*w *Res* Machine Translation Benchmark is a low-resource MT dataset.  |       
|[GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)       |   Georgi Gerganov Machine Learning, a C library focused on machine learning           |       
|[GSM8K](https://arxiv.org/pdf/2109.01152.pdf)      |	Grade School Math 8K, GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. |  
|[HNSW](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)       |	Hierarchical Navigable Small Worlds	                                                |       
|[LaMDA](https://arxiv.org/pdf/2201.08239.pdf)      |   Language Model for Dialogue Applications	                                        |       
|[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)      |	Large Language Model Meta AI	                                                    |       
|[LLaSM](https://arxiv.org/pdf/2308.15930.pdf)      |   Large Language and Speech Model                                                     |       
|LLM        |	Large Language Model: An AI model trained on mass amounts of text data to understand language and generate novel content in human-like language.	
|[MAWPS](https://github.com/sroy9/mawps)      |	A Math Word Problem Repository is an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms. | 
|ML         |	Machine Learning, A component in AI that allows computers to learn and make better predictive outcomes without explicit programming. Can be coupled with training sets to generate new content.	
|MLM        |	Masked Language Model	
|[MMLU](https://arxiv.org/abs/2009.03300)       |   Massive Multitask Language Understanding, a new test to measure a text model's multitask accuracy | 
|MRC        |   Machine Reading Comprehension  
|[MTPB](https://arxiv.org/pdf/2203.13474.pdf)       |   Multi-Turn Programming Benchmark, a benchmark consisting of 115 diverse problem sets that are factorized into multi-turn prompts | 
|[NeurIPS](https://neurips.cc/)    |	Neural Information Processing System	                                            |       
|NLP        |	Natural language processing. A branch of AI that uses machine learning and deep learning to give computers the ability to understand human language, often using learning algorithms, statistical models and linguistic rules.	
|NLG        |	Natural Language Generation. A branch of AI that uses machine learning and deep learning to generate human-like language.	
|NSP        |	Next Sentence Prediction
|[PALM](https://arxiv.org/pdf/2204.02311.pdf)       |   Pathways Language Model	                                                            |       
|[PEFT](https://huggingface.co/blog/peft)       |	Parameter Efficient Fine-Tuning	                                                    |       
|[PPO](https://arxiv.org/pdf/1707.06347.pdf)        |	Proximal Policy Optimization, foundational RL algorithm for learning from human preferences |       
|[RAG](https://arxiv.org/pdf/2005.11401.pdf)        |   Retriever-Augmented Generation is an AI framework that combines an information retrieval component with a text generation model to improve the quality of responses generated by LLMs.      |       
|[RLHF](https://arxiv.org/pdf/2305.18438.pdf)       |	Reinforcement Learning from Human Feedback	                                        |       
|[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)    |	Robustly Optimized BERT Approach	                                                |       
|[ROGUE](https://arxiv.org/abs/1803.01937)      |	Recall-Oriented Understudy for Gisting Evaluation, a metric for evaluating a generated sentence to a reference sentence. |       
|[SQuAD](https://arxiv.org/pdf/1606.05250.pdf)      |	Stanford Question Answering Dataset	                                                |       
|[SVAMP](https://github.com/arkilpatel/SVAMP)      |   Simple Variations on Arithmetic Math word Problems is a challenge set to enable more robust evaluation of automatic MWP (Math Word Problem) solvers | 
|[T5](https://huggingface.co/docs/transformers/model_doc/t5)         |	Text to Text Transfer Transformer	                                                |       
|[XLM](https://arxiv.org/pdf/1901.07291.pdf)        |	Cross-lingual Language Models	                                                    |       
|XLU        |	Cross-lingual Understanding	
|[XNLI](https://github.com/facebookresearch/XNLI)       |	Cross-lingual Natural Language Inference	                                        |       
|[XLNet](https://arxiv.org/pdf/1906.08237.pdf)      |	Generalized Autoregressive Pretraining for Language Understanding	                |       
|[ZeRO](https://arxiv.org/pdf/1910.02054.pdf)       |   *Ze*ro *R*edundancy *O*ptimizer	                                                    |       

---
Note: _PRs are accepted. Feel free to add more terms and their details._